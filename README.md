# MDP REPRESENTATION

## AIM:


## PROBLEM STATEMENT:

### Problem Description


### State Space


### Sample State


### Action Space

### Sample Action


### Reward Function


### Graphical Representation


## PYTHON REPRESENTATION:
```py
P={0: {0: [(0.7, 1, 1, False), (0.1, 0, 0.0, False), (0.1, 0, 0.0, False),(0.1, 0, 0.0, False)],
        1: [(1, 0, 0.0, False), (0, 0, 0.0, False), (0, 0, 0.0, False),(0, 0, 0.0, False)],
       2: [(1, 0, 0.0, False), (0, 0, 0.0, False), (0, 0, 0.0, False),(0, 0, 0.0, False)],
       3: [(1, 0, 0.0, False), (0, 0, 0.0, False), (0, 0, 0.0, False),(0, 0, 0.0, False)],


       },
   1:   { 0: [(1, 1, 0, False), (0, 1, 0.0, False), (0, 1, 0.0, False),(0, 1, 0.0, False)],
        1: [(0.7, 2, 1, False), (0.1, 1, 0.0, False), (0.1, 1, 0.0, False),(0.1, 1, 0.0, False)],
       2: [(1, 1, 0.0, False), (0, 1, 0.0, False), (0, 1, 0.0, False),(0, 1, 0.0, False)],
       3: [(1.7, 1, 0.0, False), (0, 1, 0.0, False), (0, 1, 0.0, False),(0, 1, 0.0, False)],


       },
  2:  {0: [(1, 2, 0, False), (0, 2, 0.0, False), (0, 2, 0.0, False),(0, 2, 0.0, False)],
        1: [(1, 2, 0.0, False), (0, 2, 0.0, False), (0, 2, 0.0, False),(0, 2, 0.0, False)],
       2: [(0.7, 3, 1, False), (0.1, 2, 0.0, False), (0.1, 2, 0.0, False),(0.1, 2, 0.0, False)],
       3: [(1, 2, 0.0, False), (0, 2, 0.0, False), (0, 2, 0.0, False),(0, 2, 0.0, False)],


       },
   3: {0: [(1, 3, 1, False), (0, 3, 0.0, False), (0, 3, 0.0, False),(0, 3, 0.0, False)],
        1: [(1, 3, 0.0, False), (0, 3, 0.0, False), (0, 3, 0.0, False),(0, 3, 0.0, False)],
       2: [(1, 3, 0.0, False), (0, 3, 0.0, False), (0, 3, 0.0, False),(0, 3, 0.0, False)],
       3: [(0.7, 0, 10, True), (0.1, 3, 0.0, False), (0.1, 3, 0.0, False),(0.1, 3, 0.0, False)],


       },

}


```

## OUTPUT:
![image](https://github.com/user-attachments/assets/101713a9-7f56-482d-898a-b6d3cf09dc58)


## RESULT:
Thus the given real world problem is successfully represented in a MDP form.

